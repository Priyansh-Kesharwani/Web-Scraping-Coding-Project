{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install markdownify bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmcA1Nt-UDRI",
        "outputId": "dfc0696d-01e8-4942-85aa-abd0822f9452"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: markdownify in /usr/local/lib/python3.10/dist-packages (0.11.6)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.9 in /usr/local/lib/python3.10/dist-packages (from markdownify) (4.11.2)\n",
            "Requirement already satisfied: six<2,>=1.15 in /usr/local/lib/python3.10/dist-packages (from markdownify) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5,>=4.9->markdownify) (2.5)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1256 sha256=83cefd8193e83f40b0dd10c41638ad397ebf0a900409319e9efce5faef38f67c\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "Successfully built bs4\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "UTVNBp_iT4ah"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import markdownify\n",
        "import os\n",
        "\n",
        "base_url = \"https://en.wikipedia.org/wiki/National_Advisory_Committee_for_Aeronautics\"\n",
        "\n",
        "def clean_text(text):\n",
        "  soup = BeautifulSoup(text, 'html.parser')\n",
        "\n",
        "  unwanted_classes = [\"sidebar-class\", \"social-media-class\"]\n",
        "  unwanted_tags = [\"script\", \"style\", \"iframe\"]\n",
        "\n",
        "  for tag in soup.find_all(class_=unwanted_classes):\n",
        "      tag.extract()\n",
        "\n",
        "  for tag in soup.find_all(unwanted_tags):\n",
        "      tag.extract()\n",
        "\n",
        "\n",
        "  cleaned_text = soup.get_text()\n",
        "\n",
        "  cleaned_text = ' '.join(cleaned_text.split())\n",
        "  return text\n",
        "\n",
        "def scrape_page(url):\n",
        "    response = requests.get(url,timeout=5)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    try:\n",
        "\n",
        "        title = soup.find('title').text\n",
        "\n",
        "        content = soup.find(id='content')\n",
        "        if content:\n",
        "            content_text = clean_text(content.text)\n",
        "        else:\n",
        "            content_text = \"Content not found on this page.\"\n",
        "\n",
        "\n",
        "        tables = content.find_all('table')\n",
        "        for table in tables:\n",
        "            markdown_table = markdownify.markdownify(str(table))\n",
        "            table.replace_with(BeautifulSoup(markdown_table, 'html.parser'))\n",
        "\n",
        "        return {'title': title, 'content': content_text}\n",
        "    except Exception as e:\n",
        "\n",
        "        print(f\"Error scraping page: {url}\")\n",
        "        print(f\"Exception: {e}\")\n",
        "        return {'title': 'Error', 'content': 'Error scraping this page.'}\n",
        "\n",
        "def get_total_pages(base_url):\n",
        "    response = requests.get(base_url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    page_links = soup.find_all('a', class_='page-numbers')\n",
        "    if page_links:\n",
        "        return len(page_links)\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def scrape_new_pages(base_url, num_pages, data_dir):\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.makedirs(data_dir)\n",
        "\n",
        "    existing_data = []\n",
        "    scraped_urls = set()\n",
        "\n",
        "    data_file = os.path.join(data_dir, 'scraped_data.json')\n",
        "    if os.path.exists(data_file):\n",
        "        with open(data_file, 'r') as json_file:\n",
        "            existing_data = json.load(json_file)\n",
        "            scraped_urls = set(item['url'] for item in existing_data)\n",
        "\n",
        "    total_pages = get_total_pages(base_url)\n",
        "    num_pages_to_scrape = min(num_pages, total_pages)\n",
        "\n",
        "    new_data = []\n",
        "    for page_num in range(1, num_pages_to_scrape + 1):\n",
        "        page_url = f\"{base_url}?page={page_num}\"\n",
        "\n",
        "        if page_url in scraped_urls:\n",
        "            print(f\"Skipping already scraped URL: {page_url}\")\n",
        "            continue\n",
        "\n",
        "        page_data = scrape_page(page_url)\n",
        "        page_data['url'] = page_url\n",
        "\n",
        "        new_data.append(page_data)\n",
        "        scraped_urls.add(page_url)\n",
        "\n",
        "    all_data = existing_data + new_data if existing_data else new_data\n",
        "    with open(data_file, 'w') as json_file:\n",
        "        json.dump(all_data, json_file, indent=4)\n",
        "\n",
        "scrape_new_pages(base_url, num_pages=5, data_dir='data')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CxaQL5CLY1T7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}